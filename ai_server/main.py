#main.py
# í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import os
import uuid

# ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import numpy as np
import cv2
import subprocess
import json
from flask import Flask, request, jsonify, send_from_directory
from flask import send_file
from tensorflow.keras.models import load_model  
from tensorflow.keras.preprocessing.sequence import pad_sequences

# ì‚¬ìš©ì ì •ì˜ ëª¨ë“ˆ
from DTW import compare_poses, compute_diff_sequence
from MoveNet import extract_keypoints_from_video
from visualize_feedback import visualize_pose_feedback, summarize_top_joints, JOINT_FEEDBACK_MAP



app = Flask(__name__)
UPLOAD_FOLDER = 'uploads'
OUTPUT_FOLDER = 'outputs'
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
os.makedirs(OUTPUT_FOLDER, exist_ok=True)


EXPECTED_LEN = {
    "cranker": 365,
    "twohand": 305,
    "thumbless": 262,
    "stroker": 300  #ì˜ˆìƒ
}

@app.route('/extract_pose', methods=['POST'])
def extract_pose():
    if 'video' not in request.files:
        return jsonify({'error': 'No video part'}), 400

    file = request.files['video']
    if file.filename == '':
        return jsonify({'error': 'No selected file'}), 400

    start = float(request.form.get('start', 0))
    end = float(request.form.get('end', 0))

    video_path = os.path.join(UPLOAD_FOLDER, file.filename)
    file.save(video_path)

    trimmed_path = os.path.join(OUTPUT_FOLDER, f"trimmed_{file.filename}")
    trim_video_opencv(video_path, trimmed_path, start, end)

    keypoints_path = extract_keypoints_from_video(trimmed_path, OUTPUT_FOLDER)

    return jsonify({
    'message': 'Pose extracted',
    'keypoints_path': keypoints_path.replace('\\', '/'),
    'trimmed_video': trimmed_path.replace('\\', '/')  # âœ… ì¶”ê°€
})

def predict_framewise_labels(diff_seq, model_path):
    pitch_type = os.path.basename(model_path).replace("lstm_", "").replace(".h5", "")
    maxlen = EXPECTED_LEN.get(pitch_type, 278)  # ê¸°ë³¸ê°’ 278

    model = load_model(model_path)
    padded = pad_sequences([diff_seq], padding='post', maxlen=maxlen)

    preds = model.predict(padded)  # shape: (1, T, 1)
    framewise = np.squeeze(preds[0])

    if framewise.ndim == 0:
        framewise = np.array([framewise])

    labels = (framewise > 0.5).astype(int).tolist()[:len(diff_seq)]
    confidence = float(np.mean(framewise[:len(diff_seq)]))

    print(f"âœ… preds shape: {preds.shape}")
    print(f"ğŸ¯ labels ìƒì„±ë¨: {len(labels)}ê°œ, confidence: {confidence:.2f}")

    return labels, confidence



@app.route('/analyze_pose', methods=['POST'])
def analyze_pose():
    try:
        test_filename = request.json.get('test_keypoints')
        pitch_type = request.json.get('pitch_type')

        #ë””ë²„ê¹… ë¡œê·¸
        print("âœ… analyze_pose ìš”ì²­ ìˆ˜ì‹ ")
        print(f"ğŸ“© ë°›ì€ test_keypoints: {test_filename}")
        print(f"ğŸ“© ë°›ì€ pitch_type: {pitch_type}")

        if not test_filename or not pitch_type:
            return jsonify({'error': 'test_keypoints ë˜ëŠ” pitch_type ëˆ„ë½'}), 400
        
        # í•œê¸€ â†’ ì˜ë¬¸ pitch_type ë§¤í•‘
        type_map = {'ìŠ¤íŠ¸ë¡œì»¤':'stroker','íˆ¬í•¸ë“œ': 'twohand', 'ë¤ë¦¬ìŠ¤': 'thumbless', 'í¬ë­ì»¤':'cranker'}
        mapped_type = type_map.get(pitch_type.strip(), pitch_type.strip())  # ë§¤í•‘ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        print(f"ğŸ“‚ pitch_type ë§¤í•‘ ê²°ê³¼: {mapped_type}")

        # ê²½ë¡œ ì •ê·œí™”
        test_path = os.path.normpath(test_filename)
        test_path = os.path.abspath(test_path)
        print(f"ğŸ“‚ ìµœì¢… ì •ê·œí™”ëœ ê²½ë¡œ: {test_path}, ì¡´ì¬ ì—¬ë¶€: {os.path.exists(test_path)}")
        
        reference_path = os.path.abspath("Data/keypoints/twohand/twohand_001.npy")
        model_path = os.path.abspath(f"lstm_{mapped_type}.h5")

        # ê²½ë¡œ ì¡´ì¬ ì—¬ë¶€ ì²´í¬ (ë¡œê¹… í¬í•¨)
        print(f"ğŸ“‚ test_path: {test_path}, exists: {os.path.exists(test_path)}")
        print(f"ğŸ“‚ reference_path: {reference_path}, exists: {os.path.exists(reference_path)}")
        print(f"ğŸ“‚ model_path: {model_path}, exists: {os.path.exists(model_path)}")

        if not os.path.exists(reference_path) or not os.path.exists(test_path) or not os.path.exists(model_path):
            return jsonify({'error': 'íŒŒì¼ ê²½ë¡œê°€ ì˜ëª»ë˜ì—ˆê±°ë‚˜ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.'}), 400

        distance, ref, test, path = compare_poses(reference_path, test_path)
        diff_seq = compute_diff_sequence(ref, test, path)

        #í”„ë ˆì„ë³„ ì˜ˆì¸¡
        labels, confidence = predict_framewise_labels(diff_seq, model_path)
        score = round(confidence * 100, 2)

        #labels ì²´í¬
        if len(labels) < 2:
            print("âš ï¸ ì‹œê°í™”ìš© labelsê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤. ë¹„êµ ì˜ìƒ ìƒëµ")
            return jsonify({
                'feedback': 'ë¶„ì„í•  ìˆ˜ ìˆëŠ” ìì„¸ ì •ë³´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.',
                'distance': round(distance, 2),
                'score': 0.0,
                'pitch_type': pitch_type,
                'comparison_video': None
            })

        # ë¹„êµ ì˜ìƒ ìƒì„± (ì—¬ê¸°ë¡œ ì´ë™)
        comparison_filename = f"comparison_{uuid.uuid4().hex}.mp4"
        comparison_path = os.path.join("outputs", comparison_filename)

        #ë™ì˜ìƒ ìœ„ì— ì„ ê·¸ë¦¬ê¸°
        source_video = request.json.get('source_video')
        if not source_video or not os.path.exists(source_video):
            return jsonify({'error': 'ì›ë³¸ trimmed ì˜ìƒ ê²½ë¡œ ëˆ„ë½ ë˜ëŠ” ì¡´ì¬í•˜ì§€ ì•ŠìŒ'}), 400

        top_joints = summarize_top_joints(diff_seq, labels, top_k=4)
        visualize_pose_feedback(ref, test, labels, diff_seq, top_joints, comparison_path, source_video=source_video)

        
        # ì •ëŸ‰ì  ê¸°ì¤€ ì¬ì¡°ì •
        wrong_ratio = sum(labels) / len(labels)

        if score >= 80 and wrong_ratio < 0.1:
            feedback_text = "ìì„¸ê°€ ë§¤ìš° ì ì ˆí•©ë‹ˆë‹¤!"
        elif score >= 60:
            feedback_text = "ì „ë°˜ì ìœ¼ë¡œ ì–‘í˜¸í•˜ë‚˜ ì•½ê°„ì˜ ë³´ì™„ì´ í•„ìš”í•©ë‹ˆë‹¤."
        else:
            feedback_lines = []
            for j in top_joints:
                if j in JOINT_FEEDBACK_MAP:
                    feedback_lines.append(JOINT_FEEDBACK_MAP[j])
                else:
                    continue  # ì„¤ëª… ì—†ëŠ” ê´€ì ˆì€ ìƒëµí•˜ê±°ë‚˜ í•„ìš” ì‹œ ë§¤í•‘ ì¶”ê°€

            feedback_text = (
                "ğŸ’¡ ì°¸ê³ : ì˜ìƒì—ì„œ 'ë¹¨ê°„ ì„ 'ì€ LSTM ëª¨ë¸ì´ ì´ìƒí•˜ë‹¤ê³  íŒë‹¨í•œ í”„ë ˆì„ ì¤‘\n"
                "'ì£¼ìš” ê´€ì ˆ'ì— í•´ë‹¹í•˜ëŠ” ë¶€ìœ„ë¥¼ ê°•ì¡°í•œ ê²ƒì…ë‹ˆë‹¤.\n\n"
                "ë‹¤ìŒ ë¶€ìœ„ì˜ ë³´ì™„ì´ í•„ìš”í•©ë‹ˆë‹¤:\n- " + "\n- ".join(feedback_lines)
            )

        return jsonify({
            'feedback': feedback_text,
            'distance': round(distance, 2),
            'score': score,
            'pitch_type': pitch_type,
            'comparison_video': comparison_filename
        })

    except Exception as e:
        print(f"âŒ ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜: {e}")
        return jsonify({'error': 'ì„œë²„ ì˜¤ë¥˜ ë°œìƒ', 'detail': str(e)}), 500


#ìŠ¤íŠ¸ë¦¬ë° mp4
@app.route('/video/<filename>')
def serve_video(filename):
    path = os.path.join('outputs', filename)
    if not os.path.exists(path):
        return jsonify({'error': 'ë¹„êµ ì˜ìƒì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.'}), 404

    print(f"ğŸ“¤ ì˜ìƒ ì „ì†¡ ì‹œì‘: {filename}")
    return send_file(path, mimetype='video/mp4', as_attachment=False)


def rotate_frame_if_needed(frame):
    h, w = frame.shape[:2]
    if w > h:
        return cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)
    return frame

def trim_video_opencv(input_path, output_path, start_time, end_time):
    cap = cv2.VideoCapture(input_path)
    if not cap.isOpened():
        print("âŒ ì›ë³¸ ì˜ìƒ ì—´ê¸° ì‹¤íŒ¨")
        return

    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    if fps == 0 or total_frames == 0:
        cap.release()
        print("âŒ FPS ë˜ëŠ” ì´ í”„ë ˆì„ ìˆ˜ê°€ 0ì…ë‹ˆë‹¤.")
        return

    start_frame = int(start_time * fps)
    end_frame = min(int(end_time * fps), total_frames - 1)

    # âœ… ì²« í”„ë ˆì„ ì½ê¸°
    ret, first_frame = cap.read()
    if not ret:
        cap.release()
        print("âŒ ì²« í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨")
        return

    rotated_first_frame = rotate_frame_if_needed(first_frame)
    height, width = rotated_first_frame.shape[:2]
    print(f"ğŸ¥ Trimëœ ì˜ìƒ í•´ìƒë„: {width}x{height} (ê°€ë¡œxì„¸ë¡œ)")

    # ë‹¤ì‹œ ì‹œì‘ ìœ„ì¹˜ë¡œ ì´ë™
    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)

    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

    frame_idx = 0
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret or frame_idx > end_frame:
            break

        if start_frame <= frame_idx <= end_frame:
            frame = rotate_frame_if_needed(frame)
            out.write(frame)

        frame_idx += 1

    cap.release()
    out.release()
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {output_path}")


if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
