#main.py
from flask import Flask, request, jsonify
import os
import uuid
import numpy as np
from MoveNet import extract_keypoints_from_video
from DTW import compare_poses, compute_diff_sequence, visualize_keypoint_diff
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences
from flask import send_from_directory
import cv2

app = Flask(__name__)
UPLOAD_FOLDER = 'uploads'
OUTPUT_FOLDER = 'outputs'
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
os.makedirs(OUTPUT_FOLDER, exist_ok=True)

@app.route('/extract_pose', methods=['POST'])
def extract_pose():
    if 'video' not in request.files:
        return jsonify({'error': 'No video part'}), 400

    file = request.files['video']
    if file.filename == '':
        return jsonify({'error': 'No selected file'}), 400

    start = float(request.form.get('start', 0))
    end = float(request.form.get('end', 0))

    video_path = os.path.join(UPLOAD_FOLDER, file.filename)
    file.save(video_path)

    trimmed_path = os.path.join(OUTPUT_FOLDER, f"trimmed_{file.filename}")
    trim_video_opencv(video_path, trimmed_path, start, end)

    keypoints_path = extract_keypoints_from_video(trimmed_path, OUTPUT_FOLDER)

    return jsonify({
    'message': 'Pose extracted',
    'keypoints_path': keypoints_path.replace('\\', '/')
})


@app.route('/analyze_pose', methods=['POST'])
def analyze_pose():
    try:
        test_filename = request.json.get('test_keypoints')
        pitch_type = request.json.get('pitch_type')

        #ë””ë²„ê¹… ë¡œê·¸
        print("âœ… analyze_pose ìš”ì²­ ìˆ˜ì‹ ")
        print(f"ğŸ“© ë°›ì€ test_keypoints: {test_filename}")
        print(f"ğŸ“© ë°›ì€ pitch_type: {pitch_type}")

        if not test_filename or not pitch_type:
            return jsonify({'error': 'test_keypoints ë˜ëŠ” pitch_type ëˆ„ë½'}), 400
        
        # í•œê¸€ â†’ ì˜ë¬¸ pitch_type ë§¤í•‘
        type_map = {'ìŠ¤íŠ¸ë¡œì»¤':'stroker','íˆ¬í•¸ë“œ': 'twohand', 'ë¤ë¦¬ìŠ¤': 'thumbless', 'í¬ë­ì»¤':'cranker'}
        mapped_type = type_map.get(pitch_type.strip(), pitch_type.strip())  # ë§¤í•‘ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        print(f"ğŸ“‚ pitch_type ë§¤í•‘ ê²°ê³¼: {mapped_type}")

        # ê²½ë¡œ ì •ê·œí™”
        test_path = os.path.normpath(test_filename)
        test_path = os.path.abspath(test_path)
        print(f"ğŸ“‚ ìµœì¢… ì •ê·œí™”ëœ ê²½ë¡œ: {test_path}, ì¡´ì¬ ì—¬ë¶€: {os.path.exists(test_path)}")
        
        reference_path = os.path.abspath("Data/keypoints/twohand/twohand_001.npy")
        model_path = os.path.abspath(f"lstm_{mapped_type}.h5")

        # ê²½ë¡œ ì¡´ì¬ ì—¬ë¶€ ì²´í¬ (ë¡œê¹… í¬í•¨)
        print(f"ğŸ“‚ test_path: {test_path}, exists: {os.path.exists(test_path)}")
        print(f"ğŸ“‚ reference_path: {reference_path}, exists: {os.path.exists(reference_path)}")
        print(f"ğŸ“‚ model_path: {model_path}, exists: {os.path.exists(model_path)}")

        if not os.path.exists(reference_path) or not os.path.exists(test_path) or not os.path.exists(model_path):
            return jsonify({'error': 'íŒŒì¼ ê²½ë¡œê°€ ì˜ëª»ë˜ì—ˆê±°ë‚˜ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.'}), 400

        distance, ref, test, path = compare_poses(reference_path, test_path)
        diff_seq = compute_diff_sequence(ref, test, path)

        # ë¹„êµ ì˜ìƒ ìƒì„± (ì—¬ê¸°ë¡œ ì´ë™)
        comparison_filename = f"comparison_{uuid.uuid4().hex}.mp4"
        comparison_path = os.path.join("outputs", comparison_filename)
        visualize_keypoint_diff(ref, test, save_path=comparison_path)

        feedback_text = predict_feedback(diff_seq, model_path)

        return jsonify({
            'feedback': feedback_text,
            'distance': round(distance, 2),
            'pitch_type': pitch_type,
            'comparison_video': comparison_filename
        })

    except Exception as e:
        print(f"âŒ ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜: {e}")
        return jsonify({'error': 'ì„œë²„ ì˜¤ë¥˜ ë°œìƒ', 'detail': str(e)}), 500


def predict_feedback(diff_seq, model_path):
    model = load_model(model_path)
    padded = pad_sequences([diff_seq], padding='post', maxlen=267, dtype='float32')
    pred = model.predict(padded)

    label_map = {0: "ìì„¸ê°€ ì ì ˆí•©ë‹ˆë‹¤!"}  # ë‹¨ì¼ í´ë˜ìŠ¤ ê¸°ì¤€ìœ¼ë¡œ ë©”ì‹œì§€ ì„¤ì •
    predicted_label = 0 if pred[0][0] > 0.5 else 1
    return label_map.get(predicted_label, "ìì„¸ê°€ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")

#ìŠ¤íŠ¸ë¦¬ë° mp4
@app.route('/video/<filename>')
def serve_video(filename):
    return send_from_directory('outputs', filename)


def trim_video_opencv(input_path, output_path, start_time, end_time):
    cap = cv2.VideoCapture(input_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    start_frame = int(start_time * fps)
    end_frame = int(end_time * fps)

    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))
    frame_idx = 0

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        if frame_idx > end_frame:
            break
        if start_frame <= frame_idx <= end_frame:
            out.write(frame)
        frame_idx += 1

    cap.release()
    out.release()
    print(f"âœ… ì˜ë¼ë‚¸ ì˜ìƒ ì €ì¥ ì™„ë£Œ: {output_path}")

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
